{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sklean](images/scikit-learn-logo.png)\n",
    "\n",
    "[scikit-learn.org](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction\n",
    "\n",
    "Scikit-learn (Sklearn) is a free open-source machine learning library in Python and is currently one of the most popular machine learning libaries on github. It provides a selection of efficient tools for machine learning & statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. [1](#References)\n",
    "\n",
    "The library is built upon the [SciPy (Scientific Python)](https://www.scipy.org/) & intergrates well with many other Python libraries, such as Matplotlib, NumPy & Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Machine learning involves the creation of computational models or algorithms from data, that tries to derive rules or procedures to explain the data or to predict future data.\n",
    "\n",
    "The more data the system is presented with, the more refined the model, the quality of the learned model is also dependent on the quality of the data used to train it. [2](#References)\n",
    "\n",
    "Three important categories of machine learning are:\n",
    "\n",
    "1. **Supervised learning** - Models which are trained with known labeled data sets, the trained algorithm produces an inferred function to make predictions about the output values for unknown input values.\n",
    "\n",
    "2. **Unsupervised learning** - Models which use data that is neither classified nor labeled, the algorithm explores the data & can draw inferences from datasets to describe hidden structures, patterns or natural groupings.\n",
    "\n",
    "3. **Reinforcement learning & deep learning** - models trained through trial and error to take the best action by establishing a reward system. Note that reinforcement learning & deep learning are currently not within the scope of the scikit-learn library as extensive knowledge to define the architecture is required along with GPUs for efficient computing. Refer to [tensorflow](https://www.tensorflow.org/), [keras](https://keras.io/) and [pytorch](https://pytorch.org/) for deep learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn Features\n",
    "The library is focused on modeling data. It is not focused on loading, manipulating and summarizing data. [NumPy](https://numpy.org/) & the [Pandas](https://pandas.pydata.org/) python libaries packages facilitate the structuring, manipulating & performance of mathematical functions or operations on large quantities of data.\n",
    "\n",
    "Some of the groups of models provided by Sklearn include:\n",
    "\n",
    "1. **Supervised Learning algorithms** − Linear Regression, Logistic Regression, Support Vector Machine (SVM), Decision Tree, Random Forest, k-nearest neighbors (KNN).\n",
    "2. **Unsupervised Learning algorithms** − K-Means, Factor Analysis, Principal Component Analysis (PCA).\n",
    "3. **Clustering** − dividing of unlabeled data into groups or clusters such that the data points within the have similar features.\n",
    "4. **Cross Validation** − checks the accuracy of supervised models on unseen data.\n",
    "5. **Dimensionality Reduction** − reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "6. **Ensemble methods** − combining the predictions of multiple supervised models.\n",
    "7. **Feature extraction** − extracts the features from data to define the attributes in image and text data.\n",
    "8. **Feature selection** − identify useful attributes to create supervised models.\n",
    "\n",
    "Scikit learn also incorporates a few small standard datasets that do not require the downloading of any files from external websites, these datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. \n",
    "\n",
    "See the [Dataset loading utilities](https://scikit-learn.org/stable/datasets.html) & [Toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets) pages on the scikit-learn website for a list of these datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Scikit Learn Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Loading:**\n",
    "\n",
    "Scikit learn contains a number of small standard datasets which can easilty be loaded through the package.\n",
    "\n",
    "There are also miscellaneous tools to load datasets of other formats or from other locations, such as loading sample images, downloading datasets from the openml.org or loading external datasets using Pandas & NumPy. Please refer to [scikit-learn - loading other datasets page](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#loading-other-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libaries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot style\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Increase the size of the output plots\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "# loading the iris dataset directly through scikit-learn\n",
    "iris = load_iris()\n",
    "\n",
    "# uncomment print() function to view full output of the iris dataset\n",
    "#print(iris)\n",
    "\n",
    "print(type(iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reviewing the Dataset**\n",
    "\n",
    "By viewing the output of the loaded iris dataset, it can be seen that the dataset is returned as a scikit learn ```Bunch object``` which is similar to a python dictionary data type consisting of keys & values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename']) \n",
      "\n",
      "feature names =  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \n",
      "\n",
      "target names =  ['setosa' 'versicolor' 'virginica']\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# Output the keys of the iris dataset\n",
    "print(iris.keys(), \"\\n\")\n",
    "\n",
    "# Output the feature names & target names\n",
    "print(\"feature names = \", iris.feature_names, \"\\n\")\n",
    "print(\"target names = \", iris.target_names)\n",
    "\n",
    "# Output the description of the iris dataset\n",
    "print(iris.DESCR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```data``` is the input variables, attributes or feature data of the dataset contained within a NumPy array.\n",
    "- ```target``` is the output variables, target or label data, which is determined by the feature variables and is also contained with an NumPy array.  \n",
    "- ```target_names``` is the list of names of the output variables.\n",
    "- ```feature_names``` is the list of all the names of the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **features matrix** is a two-dimensional array with shape ```[n_samples, n_features]```, where the samples (rows) refers to the individual objects described by the dataset and the features (columns) refers to the distinct observations or values that describe each sample. The features matrix is often stored in a variable named ```X```.\n",
    "\n",
    "In the iris dataset there are 150 samples of iris flowers with 4 features measured for each sample: sepal length, sepal width, petal length, petal width & therefore as a two-dimensional array, it will have a shape of 150 x 4.\n",
    "\n",
    "The **target array** is typically a one dimensional, with length ```n_samples``` and can consists of either continuous numerical values, or discrete classes/labels and is often stored in a variable named ```y```. The target array is usually the quantity that is to be predicted using the scikit learn machine learning algorithms. \n",
    "\n",
    "In the iris dataset the ```species``` column is the target array. The preceeding features data is used to construct a model which can predict the species of a flower sample, i.e is the measured flower sample a Setosa, Versicolour or Virginica  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# assign the features data to variable X.\n",
    "X = iris.data\n",
    "print(X.shape)\n",
    "\n",
    "# assign the target data to varaible y.\n",
    "y = iris.target\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `images/Scikit_learn_dataset.png'\n",
      "/bin/bash: -c: line 0: `[sklearn_dataset](images/Scikit_learn_dataset.png)'\n"
     ]
    }
   ],
   "source": [
    "![sklearn_dataset](images/Scikit_learn_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [www.tutorialspoint.com, Scikit Learn - Introduction](https://www.tutorialspoint.com/scikit_learn/scikit_learn_introduction.htm)\n",
    "\n",
    "[2] [Artificial Intelligence and the Future of Work, Thomas W. Malone](https://workofthefuture.mit.edu/wp-content/uploads/2020/12/2020-Research-Brief-Malone-Rus-Laubacher2.pdf)\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html\n",
    "\n",
    "https://www.educative.io/blog/scikit-learn-cheat-sheet-classification-regression-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
